{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from internvl_server import InternVL, init_model\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def is_valid_url(server_url):\n",
    "    try:\n",
    "        result = urlparse(server_url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "class InternVLLLM(LLM):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    def parse_prompt(self, prompt):\n",
    "        txt_prompt, image_path = prompt.split(\"image_url=\")\n",
    "        return txt_prompt, image_path\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given input.\n",
    "\n",
    "        Override this method to implement the LLM logic.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of the stop substrings.\n",
    "                If stop tokens are not supported consider raising NotImplementedError.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            The model output as a string. Actual completions SHOULD NOT include the prompt.\n",
    "        \"\"\"\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "\n",
    "        server = kwargs[\"server\"]\n",
    "        txt_prompt, image_path = self.parse_prompt(prompt)\n",
    "\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "        data = {\"image_url\": image_data, \"prompt\": txt_prompt}\n",
    "\n",
    "        json_data = json.dumps(data)\n",
    "\n",
    "        if is_valid_url(server):\n",
    "            response = requests.post(\n",
    "                server, data=json_data, headers={\"Content-Type\": \"application/json\"}\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                result = f\"Successfully sent {image_path}\\n. Server response: {response.json()}.\"\n",
    "            else:\n",
    "                result = f\"Failed to send {image_path}\\n. Server response: {response.text}.\"\n",
    "        elif callable(server):\n",
    "            response = server(image_data, txt_prompt)[\"output\"]\n",
    "            result = f\"Successfully sent {image_path}\\n. Server response: {response}\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid server.\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\"\n",
    "\n",
    "\n",
    "def demo(server):\n",
    "    img_folder_default = \"/home/stardust/Downloads/test_images\"\n",
    "    judge_prompt_default = \"How many bottles in the picture?\"\n",
    "\n",
    "    img_folder = img_folder_default\n",
    "    judge_prompt = judge_prompt_default\n",
    "\n",
    "    if os.path.isdir(img_folder):\n",
    "        file_patterns = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
    "        image_batch = []\n",
    "        for file_pattern in file_patterns:\n",
    "            image_batch.extend(glob.glob(os.path.join(img_folder, file_pattern)))\n",
    "        image_batch = sorted(image_batch)\n",
    "    elif os.path.isfile(img_folder):\n",
    "        image_batch = [img_folder]\n",
    "\n",
    "    llm = InternVLLLM()\n",
    "    print(llm)\n",
    "\n",
    "    batch_messages = [\n",
    "        [HumanMessage(content=judge_prompt + \"image_url=\" + image_path)]\n",
    "        for image_path in image_batch\n",
    "    ]\n",
    "\n",
    "    print(llm.batch(batch_messages, server=server))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'model_name': 'CustomChatModel'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Successfully sent /home/stardust/Downloads/test_images/image.jpeg\\n. Server response: There is one bottle in the picture.', 'Successfully sent /home/stardust/Downloads/test_images/image.png\\n. Server response: There are three bottles in the picture.']\n"
     ]
    }
   ],
   "source": [
    "# use callable server\n",
    "model_args = init_model()\n",
    "internvl = InternVL(model_args)\n",
    "demo(internvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use deployed api server\n",
    "server = \"http://172.17.0.8:8080\"  # replace your server url\n",
    "demo(server)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
